# -*- coding: utf-8 -*-
"""Fine_Tune_Gpt_Text_generation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VEtSHuy7CE4_iRtrcwrsGStvAnCFlEaV

# Problem Statement
- In this project we will fine tune the GPT-2 model on `Shek Sphere` Dataset.
"""

!pip install datasets

import torch
from torch import nn
from torchvision import transforms
from torch.utils.data import Dataset,DataLoader
from datasets import load_dataset

# Load the Shakespeare dataset
dataset = load_dataset("tiny_shakespeare")

# Train set
train_data=dataset['train']

# Test set
test_data=dataset['test']

train_data['text'][0]

len(train_data['text'])

# Load the GPT-2 Model and Tokenizer

# Load model directly
from transformers import AutoTokenizer, AutoModelForCausalLM

checkpoint='openai-community/gpt2'
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

# Test the tokenizer
encode_text = tokenizer("Hey how are you")

encode_text

tokenizer.decode(encode_text['input_ids'])

# Now Make a Custom DataSet
class CustomDataset(Dataset):
    def __init__(self,data,tokenizer):
        self.data = data
        self.tokenizer = tokenizer


    def __len__(self):
        return len(self.data)

    def __getitem__(self,idx):
        text = self.data[idx]['text']

        self.tokenizer.pad_token = tokenizer.eos_token

        tokenize_text = self.tokenizer(text,padding=True,truncation=True,return_tensors='pt')


        return {
            'input_ids':tokenize_text['input_ids'].squeeze(), # Squeeze the 'input_ids' ten,
            'attention_mask':tokenize_text['attention_mask'].squeeze(),
        }

# Train Data
custom_train_data = CustomDataset(train_data,tokenizer)

# Test Data
custom_test_data = CustomDataset(test_data,tokenizer)

len(custom_train_data)

# Data Loader
train_loader = DataLoader(custom_train_data,batch_size=8,shuffle=True)
test_loader = DataLoader(custom_test_data,batch_size=8,shuffle=True)

for batch in train_loader:
    print(batch['input_ids'].shape)
    print(batch['attention_mask'].shape)
    break

# Load the model
model = AutoModelForCausalLM.from_pretrained(checkpoint)

model

# Freeze all the layers
for param in model.parameters():
    param.requires_grad = False

# Unfreeze the last head
for param in model.lm_head.parameters():
    param.requires_grad = True

# Set the optimzier
# We donot set the loss b/c gpt model can internally calculate the loss
optimizer = torch.optim.AdamW(model.parameters(),lr=5e-5)

optimizer

device = "cuda" if torch.cuda.is_available() else "cpu"
device

from tqdm import tqdm

# Save Checkpoints
def save_checkpoint(model, optimizer, epoch, filename="checkpoint.pth.tar"):
    """
    Saves the model checkpoint.
    """
    print("Saving checkpoint...")
    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
    }
    torch.save(checkpoint, filename)

# Evulate Model
def evaluate(model, data_loader, device):
    model.eval()
    with torch.no_grad():
        test_loss=0
        correct_pred=0
        total_sample=0
        for batch in tqdm(data_loader):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)

            output = model(input_ids=input_ids,attention_mask=attention_mask,labels=input_ids)

            # calculate the loss
            loss = output.loss
            test_loss += loss.item()

            # calculate accuracy
            logits = output.logits[:, :-1].contiguous()  # Remove last token prediction
            labels = input_ids[:, 1:].contiguous()  # Shift labels for causal prediction
            predictions = torch.argmax(logits, dim=-1)

            correct_pred += (predictions == labels).sum().item()
            total_sample += labels.numel()  # Use total_sample to update, not total_samples

        avg_acc = (correct_pred/total_sample)*100
        avg_loss = test_loss/len(train_data)
        print(f"Tests Loss: {avg_loss} Test Accuracy: {avg_acc}")

# Train model
def train_model(train_data,optimizer,device,model,epochs):
    model = model.to(device)
    model.train()
    for epoch in range(epochs):
        running_loss = 0
        correct_pred=0
        total_sample=0 # Initialize total_sample here
        for batch in tqdm(train_data):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)

            output = model(input_ids=input_ids,attention_mask=attention_mask,labels=input_ids)

            # calculate the loss
            loss = output.loss
            running_loss += loss.item()

            # Backpropagation
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            # calculate accuracy
            logits = output.logits[:, :-1].contiguous()  # Remove last token prediction
            labels = input_ids[:, 1:].contiguous()  # Shift labels for causal prediction
            predictions = torch.argmax(logits, dim=-1)

            correct_pred += (predictions == labels).sum().item()
            total_sample += labels.numel()  # Use total_sample to update, not total_samples

        avg_acc = (correct_pred/total_sample)*100
        avg_loss = running_loss/len(train_data)
        print(f"Epoch: {epoch+1}, Loss: {avg_loss} Accuracy: {avg_acc}")

        # evulate the model
        evaluate(model,test_loader,device)

        # save the model
        save_checkpoint(model,optimizer,epoch)

train_model(train_loader,optimizer,device,model,50)

"""# Evulation the model"""

# evulate the model
evaluate(model,test_loader,device)

# Prediction
def predict(model, tokenizer, input_text, device, max_length=50, num_return_sequences=1, temperature=1.0, top_k=50, top_p=0.95):
    """
    Generates diverse predictions using a pre-trained transformer model.

    Args:
        model: The transformer model (e.g., AutoModelForCausalLM).
        tokenizer: The tokenizer corresponding to the model.
        input_text (str): The input text for which predictions are required.
        device: The device to run the model on (e.g., 'cuda' or 'cpu').
        max_length (int): Maximum length of generated sequences. Default is 50.
        num_return_sequences (int): Number of sequences to return. Default is 1.
        temperature (float): Sampling temperature; higher values generate more diverse outputs.
        top_k (int): Top-K sampling to limit possible token choices. Default is 50.
        top_p (float): Nucleus sampling to limit token choices by cumulative probability. Default is 0.95.

    Returns:
        List[str]: The generated texts.
    """
    model.eval()  # Set the model to evaluation mode
    with torch.no_grad():
        # Tokenize the input text
        inputs = tokenizer(
            input_text,
            return_tensors="pt",
            padding=True,
            truncation=True,
        ).to(device)

        # Generate predictions
        output = model.generate(
            input_ids=inputs['input_ids'],
            max_length=max_length,
            num_return_sequences=num_return_sequences,
            do_sample=True,  # Enable sampling
            temperature=temperature,  # Controls randomness
            top_k=top_k,  # Limits to top-k token choices
            top_p=top_p,  # Limits cumulative probability for nucleus sampling
            no_repeat_ngram_size=2,  # Prevent repetition of phrases
            early_stopping=True,  # Stop generation if EOS is predicted
        )

    # Decode and return all generated sequences
    return [tokenizer.decode(seq, skip_special_tokens=True, clean_up_tokenization_spaces=True) for seq in output]

input_text = "Romeo and Juliet: Uneasy lies the head that wears a crown"
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Generate 3 different outputs
generated_texts = predict(
    model=model,
    tokenizer=tokenizer,
    input_text=input_text,
    device=device,
    max_length=50,
    num_return_sequences=3,
    temperature=1.2,
    top_k=50,
    top_p=0.9
)

generated_texts[0]



# Save the model
model.save_pretrained('/content/Model')
tokenizer.save_pretrained('/content/Model')

# load token from env
from google.colab import userdata
userdata.get('hf_token')

model.push_to_hub("sami606713/fine_tune_gpt",token=userdata.get('hf_token'))
tokenizer.push_to_hub("sami606713/fine_tune_gpt",token=userdata.get('hf_token'))



"""# Test the fine tune model"""

# Use a pipeline as a high-level helper
from transformers import pipeline

pipe = pipeline("text-generation", model="sami606713/fine_tune_gpt")

# Example for prediction
sequences = pipe(
    "Romeo and Juliet: ",
    max_length=50,
    num_return_sequences=3,
    do_sample=True,
    temperature=0.7
)

sequences[0]['generated_text']



